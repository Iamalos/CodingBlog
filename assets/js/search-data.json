{
  
    
        "post0": {
            "title": "Fully Connected Layer",
            "content": "Foward and backward passes . def get_data(): path = datasets.download_data(MNIST_URL, ext=&#39;.gz&#39;) with gzip.open(path, &#39;rb&#39;) as f: (x_train, y_train),(x_valid, y_valid), _ = pickle.load(f, encoding=&#39;latin-1&#39;) return map(tensor, (x_train,y_train, x_valid,y_valid)) def normalize(x,m,s): return (x-m) / s . Let&#39;s grab for this example MNIST data. MNIST data is a popular dataset for machine learning and deep learning and conists of handwritted digits from 0 to 9. By modern standards the dataset is considered to be trivial with many algorithms reaching &gt;99% test accuracy. . MNIST has a training set of 60,000 examples, and a test set of 10,000 examples. . There are many different ways to grab MNIST dataset. We leverage FastAI datasets . x_train, y_train, x_valid, y_valid = get_data() . x_train.shape, x_valid.shape . (torch.Size([50000, 784]), torch.Size([10000, 784])) . Our training set has 50 000 observations, each observation has 784 data points. This represents flattened 28 x 28 pixels of the resulting image. We can convert 784 vector back to a matrix form and plot it for vizualization . mpl.rcParams[&#39;image.cmap&#39;] = &#39;gray&#39; plt.imshow(x_train[0].view(28,28)); . Preparing the data . As a usual first step we normalize our data - subtract the mean and divide by the standard deviation of the train dataset. it is important to use the same mean and standard deviation while normalizing training and validation / test data . train_mean, train_std = x_train.mean(), x_train.std() train_mean, train_std . (tensor(0.1304), tensor(0.3073)) . # normalizing the data x_train = normalize(x_train, train_mean, train_std) x_valid = normalize(x_valid,train_mean, train_std) . def test_near_zero(a, tol=1e-3): assert a.abs() &lt; tol, f&quot;Near zero: {a}&quot; . test_near_zero(x_train.mean()) test_near_zero(1 - x_train.std()) . n,m = x_train.shape c = y_train.max() + 1 n,m,c . (50000, 784, tensor(10)) . Starting version . Let&#39;s define the number of hidden layers . nh = 50 . Initialization of weights is crucial to the training of the neural net. Using poor initialization could lead to very slow convergence of the loss function or even exploding. If weights are below 1, they will get smaller and smaller until they almost vanish and does not contribute to network learning. . We can look at a simplified example, inspired by post, to understand why weights are important. If our input for activation z is near zero, sigmoid activation function degenerates to a linear one, which means it does not bring any new information. On the other hand, if z becomes too big or too small, activation function is flat at these areas and its gradient is approaching zero. We want the variance and bias for each layer to remain stable. . We often don&#39;t worry about this as deep learning frameworks implement the necessary weights initialization under the hood, but it is still important to understand why it matters this much. I discuss weight initialization in greater details in separate posts. . #collapse seaborn.set(style=&#39;ticks&#39;) z = torch.linspace(-6,6,100) sigmoid = [1 / (1 + math.exp(-x)) for x in z] fig, ax = plt.subplots() ax.plot(z, sigmoid, c=&#39;r&#39;, label = r&quot;$ frac{1}{1+e^{- z}}$&quot;) ax.grid(True, which=&#39;both&#39;) seaborn.despine(ax=ax, offset=0) ax.spines[&#39;left&#39;].set_position(&#39;zero&#39;) ax.legend() plt.setp(ax.get_legend().get_texts(), fontsize=&#39;22&#39;); . . The goal is for each layer to have mean of 0.0 and variance of 1.0 for every layer in order to avoid gradient vanishing or exploding. This issues was tackled by Xavier in his paper Understanding the difficulty of training deep feedforward neural networks. . Let&#39;s briefly discuss the reasoning behind his approach. Consider a fully-connected linear layer: $$ y = x * w + b$$ which is $$ y = x_{1}*w_{1} + x_{2}*w_{2} + ... + x_{N}*w_{N} + b $$. . Our goal is to make the variance of $y$ equal to $1$. Assuming independence between $x$ and $y$ we have the following formula: . $$Var(x*y) = Var(x)*Var(y) + (Var(x)*E(y))^2 + (Var(y)*E(x))^2$$ . In our case $w_{i}$ was drawn from normal distrubution with zero mean and $x$ were normalized , thus for each i-th term we have: . $$ Var(x_{i}*w_{i}) = Var(x_{i}) * Var(w_{i}) + (Var(x_{i})*0)^2 + (1 * 0) ^2 = Var(x_{i}) * Var(w_{i})) $$ . We have N identically distributed elements, thus: $$ Var(y) = sum_{i=1}^{N} Var(x_{i}) * Var(w_{i}) = N * Var(x_{i}) * Var(w_{i})) $$ . We want our input to the activation function ($y$) to have the same variance as the previous layer ($x$) in order to have stability in the system. This implies, that . $$ N * Var(w_{i}) = 1 $$ $$or$$ $$ Var(w_{i}) = 1 / N $$ . This is what is called Xavier initialization. Let&#39;s proceed with it below. . # Xavier initialization w1 = torch.randn(m, nh) / math.sqrt(m) b1 = torch.zeros(nh) w2 = torch.randn(nh,1) / math.sqrt(nh) b2 = torch.zeros(1) . # check that mean and variance are equal to 0 and 1 test_near_zero(w1.mean()) test_near_zero(w1.std() - 1/math.sqrt(m)) . Let&#39;s define a simple linear layer: . def lin(x,w,b): return x @ w + b . t = lin(x_train, w1, b1) . t.mean(), t.std() . (tensor(0.0198), tensor(1.0244)) . As we see, after applying linear layer, we succeeded in our goal - mean and variance of the input to activation function is 0 and 1 (almost). But this is not the end of the story - we have to actuall pass the results through the activation function. And the results itself will be the input to the next layer. . def relu(x): return x.clamp_min(0) . t = relu(lin(x_train, w1, b1)) t.mean(), t.std() . (tensor(0.4137), tensor(0.5895)) . After applying relu our mean and variance are distorted - relu clamps all negative values to zero, thus reducting variance by half and shifting mean by 0.5. We have to take into account when initializing the weigths. This is addressed by using kaiming initialization. . # kaiming init for relu w1 = torch.randn(m, nh) * math.sqrt(2/m) . t = relu(lin(x_train, w1, b1)) t.mean(), t.std() . (tensor(0.5944), tensor(0.8434)) . This is still not perfect, but is much better than the inital approach. We could also subtract 0.5 to shift mean back to 0 (relu clamped the negative values and thus distorted the mean upwards). This initialization is implemented for us in PyTorch: . w1 = torch.zeros(m, nh) init.kaiming_normal_(w1, mode=&#39;fan_out&#39;) t = relu(lin(x_train,w1,b1)) . t.mean(), t.std() . (tensor(0.4932), tensor(0.7707)) . Unsirprisingly we get basically the same results. Note additional parameter - fan_out.This parameter specified either to preserve the magnitude of variance of the weights either in the forward pass (fan_in) or in the backwards pass (fan_out) . # as discussed above, we can subtract 0.5 from our relu to get mean closer to zero def relu(x): return x.clamp_min(0) - 0.5 . Our simple forward pass can be specified as follows: . def model(xb): l1 = lin(x_train, w1, b1) l2 = relu(l1) l3 = lin(l2, w2, b2) return l3 . %timeit -n 10 model(x_train) . 29.6 ms ± 4.9 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) . Loss function: MSE . To keep things simple, we will consider MSE loss function, although in our case it does not make any practical sense . def mse(output, targ): return (output.squeeze(-1)-targ).pow(2).mean() . preds=model(x_train) mse(preds, y_train) . tensor(28.7334) . Gradients and backward pass . Now we approach the backward pass and need to calculate the gradients of each pass. Let&#39;s start from MSE . def mse_grad(inp, targ): &quot;&quot;&quot; we accumulate gradients during the backward pass First, calculate the gradient of loss with respect to its input, which is the output of the previous layer &quot;&quot;&quot; inp.g = 2. * (inp.squeeze(-1) - targ).unsqueeze(-1) / inp.shape[0] . def relu_grad(inp, out): # gradient of relu with respect to its input layer inp.g = (inp.float() &gt; 0) * out.g . def lin_grad(inp, out, w, b): # grad of matmul with respect to input inp.g = out.g @ w.t() #w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0) w.g = inp.t() @ out.g b.g = out.g.sum(0) . def forward_and_backward(inp, targ): # forward pass: l1 = inp @ w1 + b1 l2 = relu(l1) out = l2 @ w2 + b2 # not necessary to compute loss loss = mse(out, targ) # backward pass: mse_grad(out, targ) lin_grad(l2, out, w2, b2) relu_grad(l1, l2) lin_grad(inp, l1, w1, b1) . forward_and_backward(x_train, y_train) . # save our gradients w1g = w1.g.clone() b1g = b1.g.clone() w2g = w2.g.clone() b2g = b2.g.clone() inp = x_train.g.clone() . # we can check our results against pytorch w12 = w1.clone().requires_grad_(True) b12 = b1.clone().requires_grad_(True) w22 = w2.clone().requires_grad_(True) b22 = b2.clone().requires_grad_(True) xt2 = x_train.clone().requires_grad_(True) . As pytorch calculates the backwards pass for us, we need only a forward pass . def forward(inp, targ): l1 = inp @ w12 + b12 l2 = relu(l1) l3 = l2 @ w22 + b22 return mse(l3, targ) . loss = forward(xt2, y_train) . loss.backward() . test_near(w22.grad, w2g) test_near(b22.grad, b2g) test_near(w12.grad, w1g) test_near(b12.grad, b1g) test_near(xt2.grad, inp) . So we have just checked that our gradients are correct, but the code itself is very clunky. Let&#39;s refactor it by using classes. . Refactor model . Layers as classes . class Relu(): # __call__ allows us to call Relu directly as a function def __call__(self,inp): self.inp = inp self.out = inp.clamp_min(0.) - 0.5 return self.out def backward(self): self.inp.g = (self.inp &gt; 0.).float() * self.out.g . class Lin(): def __init__(self, w, b): self.w, self.b = w, b def __call__(self, inp): self.inp = inp self.out = inp @ self.w + self.b return self.out def backward(self): #print(f&quot;out {self.out.g.shape}, w shape {self.w.shape}&quot;) self.inp.g = self.out.g @ self.w.t() self.w.g = self.inp.t() @ self.out.g self.b.g = self.out.g.sum(0) . class MSE(): def __call__(self, inp, targ): self.inp = inp self.targ = targ self.out = (inp.squeeze() - targ).pow(2).mean() def backward(self): self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.inp.shape[0] . class Model(): def __init__(self, w1, b1, w2, b2): self.layers = [Lin(w1, b1), Relu(), Lin(w2, b2)] self.loss = MSE() def __call__(self, x, targ): for l in self.layers: x = l(x) return self.loss(x, targ) def backward(self): self.loss.backward() for l in reversed(self.layers): l.backward() . w1.g,b1.g,w2.g,b2.g = [None]*4 model = Model(w1, b1, w2, b2) . %time loss = model(x_train, y_train) . CPU times: user 147 ms, sys: 83.6 ms, total: 230 ms Wall time: 31.8 ms . %time model.backward() . CPU times: user 305 ms, sys: 305 ms, total: 609 ms Wall time: 76.6 ms . test_near(w2g, w2.g) test_near(b2g, b2.g) test_near(w1g, w1.g) test_near(b1g, b1.g) test_near(inp, x_train.g) . Module.forward() . We want to get rid of the unnecessary calls to __call__ each time . class Module(): def __call__(self, *args): self.args = args self.out = self.forward(*args) return self.out def forward(self): raise Exception(&quot;not implemented&quot;) def backward(self): self.bwd(self.out, *self.args) . class Relu(Module): def forward(self, inp): return inp.clamp_min(0.)-0.5 def bwd(self, out, inp): inp.g = (inp &gt; 0).float() * out.g . class Lin(Module): def __init__(self, w, b): self.w, self.b = w, b def forward(self, inp): return inp @ self.w + self.b def bwd(self, out, inp): inp.g = out.g @ self.w.t() self.w.g = inp.t() @ out.g self.b.g = out.g.sum(0) . class MSE(Module): def forward(self, inp, targ): return (inp.squeeze(-1)-targ).pow(2).mean() def bwd(self, out, inp, targ): inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0] . class Model(): def __init__(self): self.layers = [Lin(w1, b1), Relu(), Lin(w2, b2)] self.loss = MSE() def __call__(self, x, targ): for l in self.layers: x = l(x) return self.loss(x, targ) def backward(self): self.loss.backward() for l in reversed(self.layers): l.backward() . w1.g, b1.g, w2.g, b2.g = [None] * 4 model = Model() . %time loss = model(x_train ,y_train) . CPU times: user 176 ms, sys: 82.6 ms, total: 258 ms Wall time: 34.4 ms . %time model.backward() . CPU times: user 298 ms, sys: 320 ms, total: 618 ms Wall time: 84.3 ms . test_near(w2g, w2.g) test_near(b2g, b2.g) test_near(w1g, w1.g) test_near(b1g, b1.g) test_near(inp, x_train.g) . nn.Linear and nn.Module . Now that we have an understanding of how this works, we can switch to pytorch modules - nn.Linear and nn.Module . class Model(nn.Module): def __init__(self, n_in, nh, n_out): super().__init__() self.layers = [nn.Linear(n_in, nh), nn.ReLU(), nn.Linear(nh, n_out)] self.loss = mse def __call__(self, x, targ): for l in self.layers: x = l(x) return self.loss(x.squeeze(), targ) . model = Model(m, nh, 1) . %time loss = model(x_train, y_train) . CPU times: user 150 ms, sys: 38.2 ms, total: 188 ms Wall time: 26.3 ms . %time loss.backward() . CPU times: user 258 ms, sys: 39 ms, total: 297 ms Wall time: 37.9 ms .",
            "url": "https://iamalos.github.io/CodingBlog/fastpages/deeplearning/2020/07/11/FullyConnectedLayer.html",
            "relUrl": "/fastpages/deeplearning/2020/07/11/FullyConnectedLayer.html",
            "date": " • Jul 11, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://iamalos.github.io/CodingBlog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://iamalos.github.io/CodingBlog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://iamalos.github.io/CodingBlog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://iamalos.github.io/CodingBlog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}